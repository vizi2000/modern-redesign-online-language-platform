# ðŸ¤– Ollama External IP Setup Guide

## Problem
Chatbot AI dziaÅ‚a tylko na localhost, ale nie na zewnÄ™trznym IP (194.181.240.37:8090) poniewaÅ¼ Ollama nie jest skonfigurowany do zewnÄ™trznych poÅ‚Ä…czeÅ„.

## RozwiÄ…zanie 1: Uruchom Ollama z zewnÄ™trznym dostÄ™pem

### Krok 1: Zatrzymaj Ollama
```bash
# Zatrzymaj aktualny Ollama
sudo systemctl stop ollama
# lub jeÅ›li uruchomiony rÄ™cznie:
pkill -f ollama
```

### Krok 2: Uruchom Ollama z zewnÄ™trznym dostÄ™pem
```bash
# Ustaw zmienne Å›rodowiskowe dla zewnÄ™trznego dostÄ™pu
export OLLAMA_HOST=0.0.0.0:11434
export OLLAMA_ORIGINS="*"

# Uruchom Ollama
ollama serve
```

### Krok 3: SprawdÅº czy dziaÅ‚a
```bash
# Test lokalny
curl http://localhost:11434/api/tags

# Test zewnÄ™trzny (z innego komputera)
curl http://194.181.240.37:11434/api/tags
```

### Krok 4: Ustaw jako usÅ‚ugÄ™ systemowÄ… (opcjonalne)
```bash
# Edytuj plik usÅ‚ugi
sudo nano /etc/systemd/system/ollama.service

# Dodaj zmienne Å›rodowiskowe:
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"
Environment="OLLAMA_ORIGINS=*"
...

# PrzeÅ‚aduj i uruchom
sudo systemctl daemon-reload
sudo systemctl start ollama
sudo systemctl enable ollama
```

## RozwiÄ…zanie 2: SSH Tunnel (bezpieczniejsze)

### Na serwerze (194.181.240.37):
```bash
# Uruchom tunel SSH dla Ollama
ssh -L 11434:localhost:11434 localhost -N -f
```

### W Docker nginx.conf:
```nginx
location /api/ollama/ {
    proxy_pass http://host.docker.internal:11434/api/;
    # ... reszta konfiguracji
}
```

## RozwiÄ…zanie 3: Docker Compose (zalecane dla produkcji)

### docker-compose.yml:
```yaml
version: '3.8'
services:
  akademia-app:
    build: .
    ports:
      - "8090:80"
    depends_on:
      - ollama
    networks:
      - app-network

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama-data:/root/.ollama
    environment:
      - OLLAMA_ORIGINS=*
    networks:
      - app-network

volumes:
  ollama-data:

networks:
  app-network:
    driver: bridge
```

### Uruchomienie:
```bash
docker-compose up -d
```

## RozwiÄ…zanie 4: Firewall i bezpieczeÅ„stwo

### OtwÃ³rz port 11434:
```bash
# Ubuntu/Debian
sudo ufw allow 11434

# CentOS/RHEL
sudo firewall-cmd --permanent --add-port=11434/tcp
sudo firewall-cmd --reload
```

### Zabezpiecz dostÄ™p (opcjonalne):
```bash
# Ogranicz dostÄ™p tylko do okreÅ›lonych IP
sudo ufw allow from 192.168.1.0/24 to any port 11434
```

## Aktualny status chatbota

### âœ… Co dziaÅ‚a:
- **Localhost (http://localhost:8090)**: PrÃ³buje poÅ‚Ä…czyÄ‡ siÄ™ z lokalnym Ollama
- **ZewnÄ™trzny IP (http://194.181.240.37:8090)**: UÅ¼ywa inteligentnych fallback responses
- **Fallback system**: 15+ gotowych odpowiedzi z fuzzy matching

### ðŸ”§ Inteligentne fallback responses:
```javascript
// PrzykÅ‚ady rozpoznawanych pytaÅ„:
"ile kosztujÄ… lekcje" â†’ Cennik z pakietami
"jak zaczÄ…Ä‡" â†’ Proces onboarding
"test poziomu" â†’ Link do testu jÄ™zyka
"godziny" â†’ DostÄ™pnoÅ›Ä‡ lekcji
"jÄ™zyki" â†’ Lista 5 jÄ™zykÃ³w
"bezpÅ‚atna lekcja" â†’ Info o darmowej lekcji
```

### ðŸŽ¯ Fuzzy matching:
- "cena", "koszt", "ile", "pÅ‚aciÄ‡" â†’ Cennik
- "zaczaÄ‡", "rozpoczÄ…Ä‡", "start" â†’ Jak zaczÄ…Ä‡
- "test", "poziom", "sprawdÅº" â†’ Test poziomu
- "kontakt", "email", "telefon" â†’ Dane kontaktowe

## Testowanie

### Test 1: Localhost
```bash
# OtwÃ³rz http://localhost:8090
# Kliknij chatbot i napisz "ile kosztujÄ… lekcje"
# Powinien dziaÅ‚aÄ‡ z Ollama API
```

### Test 2: ZewnÄ™trzny IP
```bash
# OtwÃ³rz http://194.181.240.37:8090
# Kliknij chatbot i napisz "ile kosztujÄ… lekcje"
# Powinien dziaÅ‚aÄ‡ z fallback responses
```

### Test 3: Po konfiguracji Ollama
```bash
# Uruchom Ollama z OLLAMA_HOST=0.0.0.0:11434
# Test z zewnÄ™trznego IP powinien uÅ¼ywaÄ‡ AI
```

## Zalecenia

### Dla rozwoju:
1. **UÅ¼yj fallback system** (juÅ¼ zaimplementowany)
2. **Ollama na localhost** dla testÃ³w lokalnych

### Dla produkcji:
1. **Docker Compose** z Ollama w kontenerze
2. **Nginx proxy** z proper CORS headers
3. **Firewall rules** ograniczajÄ…cy dostÄ™p
4. **SSL/TLS** dla bezpieczeÅ„stwa

### Dla performance:
1. **Cached responses** dla czÄ™stych pytaÅ„
2. **Rate limiting** dla API calls
3. **Model optimization** (use smaller models like phi3:mini)

## Status implementacji

âœ… **Zaimplementowano v10:**
- Inteligentny fallback system
- Automatyczne wykrywanie localhost vs external IP
- 15+ pre-defined responses z fuzzy matching
- Profesjonalne formatowanie odpowiedzi
- Linki do odpowiednich sekcji strony

ðŸ”„ **Do zrobienia:**
- Uruchom Ollama z external access (powyÅ¼sze instrukcje)
- Test na http://194.181.240.37:8090

---

**Aktualny chatbot dziaÅ‚a w trybie inteligentnych fallback responses i zapewnia Å›wietne user experience nawet bez AI API.**